<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ReferIt3D</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="shortcut icon" href="img/favicon.png">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
<div class="container" id="main">
    <div class="row">

        <h2 class="col-md-12 text-center" style="padding-bottom:20px">

            <b>ReferIt3D: Neural Listeners for Fine-Grained<br>3D Object Identification in Real-World
                Scenes</br></b>
            <span style="font-size:18pt"> ECCV 2020, Oral </span>
            <br>
        </h2>

    </div>
    <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline" style="font-size:18pt">
                <li>
                    <a href="http://optas.github.io">
                        Panos Achlioptas
                    </a>
                    </br>Stanford University
                </li>
                <li>
                    <a href="http://aabdelreheem.me">
                        Ahmed Abdelreheem
                    </a>
                    </br>KAUST
                </li>
                <li>
                    <a href="http://fxia.me/">
                        Fei Xia
                    </a>
                    </br>Stanford University
                </li>

                <br>

                <li>
                    <a href="http://www.mohamed-elhoseiny.com/">
                        Mohamed Elhoseiny
                    </a>
                    </br>Stanford University, KAUST
                </li>
                <li>
                    <a href="https://geometry.stanford.edu/member/guibas/">
                        Leonidas Guibas
                    </a>
                    </br>Stanford University
                </li>
            </ul>
        </div>
    </div>


    <div class="row" style="padding-top:45px">
        <div class="col-md-6 col-md-offset-3 text-center">
            <ul class="nav nav-pills nav-justified">
                <li>
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf">
                        <h4><strong>[Paper]</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="#video">
                        <h4><strong>[Video]</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/referit3d/referit3d">
                        <h4><strong>[Code]</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="#dataset">
                        <h4><strong>[Dataset]</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="#benchmarks">
                        <h4><strong>[Benchmarks]</strong></h4>
                    </a>
                </li>
            </ul>
        </div>
    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3 style="text-align: center;">
                <b>Abstract</b>
            </h3>
            <hr>
            <p class="text-justify">
                In this work we study the problem of using referential language to identify common objects in
                real-world
                3D scenes. We focus on a challenging setup where the referred object belongs to a fine-grained
                object
                class and the underlying scene contains multiple object instances of that class. Due to the scarcity
                and
                unsuitability of existent 3D-oriented linguistic resources for this task, we first develop two
                large-scale and complementary visio-linguistic datasets: i) Sr3D, which contains 83.5K
                template-based
                utterances leveraging spatial relations among fine-grained object classes to localize a referred
                object
                in a scene, and ii) Nr3D which contains 41.5K natural, free-form, utterances collected by deploying
                a
                2-player object reference game in 3D scenes. Using utterances of either datasets, human listeners
                can
                recognize the referred object with high (>86%, 92% resp.) accuracy. By tapping on the introduced
                data,
                we develop novel neural listeners that can comprehend object-centric natural language and identify
                the referred
                object directly in a 3D scene. Our key technical contribution is designing an approach for combining
                linguistic and geometric information (in the form of 3D point clouds) and creating multi-modal (3D)
                neural listeners. We also show that architectures which promote object-to-object communication via
                graph
                neural networks outperform less context-aware alternatives, and that fine-grained object
                classification
                is a bottleneck for language-assisted 3D object identification.
            </p>
        </div>
    </div>


    <div class="row" id="video" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3 style="text-align: center;">
                <b>Video</b>
            </h3>
            <hr>
            <!--            <video id="v0" width="100%" loop="" muted="" controls="">-->
            <!--                <source src="img/hi_res.mp4" type="video/mp4">-->
            <!--            </video>-->
            <iframe width="100%" height="400" src="https://www.youtube.com/embed/yEdf24hF_sY">
            </iframe>

        </div>

    </div>

    <div class="row" id="intuitions" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3 style="text-align: center;">
                <b>Overview & Intuitions</b>
            </h3>
            <hr>
        </div>

        <!-- Question 1-->
        <section>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h4 class="">
                        <i><b>I.</b> You focus on contrasting explicitly objects of the same <b>fine-grained</b> object
                            class <b>only</b>.<a style="color:red;"><i> <b>Why?</b></i></a></i>
                    </h4>

                    <figure>

                        <img src="img/fig1.png"
                             style="padding-bottom:1vh; padding-left: 3vw; height: 30vh; align-content: center"
                             class="img-responsive" alt="overview">
                        <figcaption>
                        </figcaption>
                    </figure>

                    <p  class="text-justify">
                         <a style="color:green;"><b>Because</b></a> it enables the human (or neural) speaker to use <u><b>minimal</b></u> details to disambiguate the “Target,”
                        thus it promotes the use of <b>efficient & fine-grained</b> Visio-linguistic reference.
                        If you compare an armchair chair to another target chair, an office chair, you can denote “the office chair.”
                        Also, the <b>explicit</b> bounding boxes that include the contrasting context help an annotator <b>focus</b> on the task,
                        especially since the ScanNet 3D/Point-Cloud reconstructions are far from smooth.
                    </p>

                </div>
            </div>
        </section>

        <!-- Question 2-->
        <section>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <br>
                    <br>
                    <h4 class="">
                        <i><b>II.</b> You contrast objects for which <b>at least one</b> same-fine-grained distractor exists in
                            the scene. <a style="color:red;"><i> <b>Why?</b></i></a></i>
                    </h4>

                    <p  class="text-justify">
                         <a style="color:green;"><b>Because</b></a> similarly to the above, this forces the reference to <u><b>go beyond</b></u> fine-grained or
                        simple-object classification! I.e., if the target is the only refrigerator of the scene, the
                        reference: “the refrigerator” is good enough, no?
                    </p>

                </div>
            </div>
        </section>


        <!-- Question 3-->
        <section>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <br>
                    <br>
                    <h4 class="">
                        <i><b>III.</b> You collected a large dataset, Nr3D which natural language. <a style="color:red;"><i> <b>Why&nbsp;</b></i></a></i> bother making also a
                            synthetic one focusing on spatial-relations?
                    </h4>

                    <figure>
                        <img src="img/ezgif.com-gif-maker.gif"
                             style="padding-bottom:1vh; padding-left: 7vw; height: 30vh; align-content: center"
                             class="img-responsive" alt="overview">
                        <figcaption>
                        </figcaption>
                    </figure>

                    <p  class="text-justify">
                        <a style="color:green;"><b>Because</b></a> as we verified in Nr3D, spatial-reasoning ("left of," "between," etc. See the above figure) is  <u><b>ubiquitous</b></u>  in natural reference.
                        Sr3D focuses on that aspect <b>only</b> — <b>disentangling</b> the reference problem nicely from other object properties such as their color or shape.
                        Also, even (naively) combining Sr3D with Nr3D in the training mix improved the listeners' performance in natural language!
                    </p>

                </div>
            </div>
        </section>

        <!-- Question 4-->
        <section>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <br>
                    <br>
                    <h4 class="">
                        <i><b>IV. </b>  Are all scenes, language, contrasting-contexts qualitatively about the same? Or, there are
                            important intrinsic differences among them?</i>
                    </h4>

                    <figure>

                        <img src="img/fig3.png"
                             style="padding-bottom:1vh; padding-left: 1vw; height: 30vh; align-content: center"
                             class="img-responsive" alt="overview">
                        <figcaption>
                        </figcaption>
                    </figure>

                    <p  class="text-justify">
                        No, they are not the same. For instance, you might contrast a lamp with a single lamp or with four of them (see the above Figure, “easy” vs. “hard”).
                        Or, you can use in your language enough elements that allow another person (or a robot!) to find the target item among all (!) objects of the scene,
                        making the reference—object pair “Scene-Discoverable” (the references that have a “tick” on the SD). Last, it is different from uttering an object-reference
                        having a specific view in mind, e.g., “The lamp on the right, in between the beds,” which implies that the listener needs to find the front-face of the bed Vs.
                        Making a reference like “The lamp closer to the white armchair,” which in theory only requires one to pinpoint the armchair without caring about its
                        front-back views. I.e., it is view-independent (“VI”).
                    </p>

                </div>
            </div>
        </section>
    </div>


    <div class="row" id="dataset" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3 style="text-align: center;">
                <b>Dataset</b>
            </h3>
            <hr>
            <ul>
                <li> You can download Nr3D <a
                        href="https://drive.google.com/file/d/1qswKclq4BlnHSGMSgzLmUu8iqdUXD8ZC/view">here</a>
                    (10.7MB) and Sr3D/Sr3D+ <a
                            href="https://drive.google.com/drive/folders/1DS4uQq7fCmbJHeE-rEbO8G1-XatGEqNV">here</a>
                    (19MB / 20MB).
                </li>
            </ul>
            <h4 style="text-align: center;">
                <b>Browse</b>
            </h4>
            <ul>
                <li> You can <strong>explore</strong> the Nr3D and Sr3D utterances inside the 3D ScanNet Scenes here
                    (<a href="./data_browser.html">Nr3D browser</a>, <a href="./sr_data_browser.html"> Sr3D
                        browser</a>).
                </li>
                <br>
                <br>
                 <figure>
                <img src="img/browser.gif" style="padding-bottom:10px; padding-left: 2vw" class="img-responsive" alt="overview">
                <figcaption>
                </figcaption>
            </figure>
            </ul>

            <br>
        </div>
    </div>

    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3 style="text-align: center;">
                <b>Method: ReferIt3DNet</b>
            </h3>
            <hr>
        </div>

        <div class="col-md-8 col-md-offset-2">
            <figure>
                <img src="img/method.png" style="padding-bottom:10px" class="img-responsive" alt="overview">
                <figcaption>
                </figcaption>
            </figure>
        </div>

        <div class="col-md-8 col-md-offset-2">
            Each object of a 3D scene, represented as a 6D point cloud containing its xyz coordinates and RGB color,
            is encoded by a visual encoder (e.g., PointNet++), with shared weights. Simultaneously, the utterance
            describing the referred object (e.g., “<i>the armchair next to the whiteboard</i>”) is processed by a
            Recurrent Neural
            Network (RNN). The resulting representations are fused together and processed by a Dynamic Graph
            Convolution
            Network (DGCN) which creates an object-centric <i>and</i> scene- (context-) aware representation per
            object.
            The output of the DGCN is processed by an MLP classifier that estimates the likelihood of each object to
            be
            the referred one. Two auxiliary losses modulate the unfused representations before these are processed
            by the DGCN
            via an object-class classifier and a text classifier respectively.
        </div>
    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3 style="text-align: center;">
                <b>Qualitative Results</b>
            </h3>
            <hr>
        </div>

        <div class="col-md-8 col-md-offset-2">
            <figure>
                <img src="img/listener_qualitative_res.png" style="padding-bottom:10px" class="img-responsive"
                     alt="overview">
                <figcaption>
                </figcaption>
            </figure>
        </div>

        <div class="col-md-8 col-md-offset-2">
            Successful cases of applying <b><i>ReferIt3DNet</i></b> are shown in the top four images and failure
            ones in the bottom two. Targets are shown in <span style="color:green">green</span> boxes, intra-class
            distractors in <span style="color:red">red</span>, and the referential text is displayed under each
            image.
            The network predictions are shown inside dashed <span style="color:yellow">yellow</span> circles, along
            with the
            inferred probabilities. We omit the probabilities of inter-class distractors to ease the presentation.
        </div>
    </div>

    <div class="row" id="citation" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3 style="text-align: center;">
                <b>Citation</b>
            </h3>
            <hr>
            If you find our work useful in your research, please consider citing:
            <pre class="w4-panel w4-centerbar w4-light-grey" style="font-size: 11px">
@inproceedings{achlioptas2020referit_3d,
    title={{ReferIt3D}: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes},
    author={Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas J.},
    booktitle={16th European Conference on Computer Vision (ECCV)},
    year={2020}
}</pre>
        </div>
    </div>

    <div class="row" id="benchmarks" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3 style="text-align: center;">
                <b>ReferIt3D Benchmark Challenges</b>
            </h3>
            <hr>
            We wish to <i>aggregate and highlight</i> results from different approaches tackling the problem of
            fine-grained 3D object identification via language. If you use either of our datasets with a new method,
            please let us know! so we can <u>add your method</u> and attained results in our <a
                href="./benchmarks.html"> benchmark-aggregating page</a>.
        </div>

    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3 style="text-align: center;">
                <b>Acknowledgements</b>
            </h3>
            <hr>
            <p class="text-justify">
                The authors wish to acknowledge the support of a Vannevar Bush Faculty Fellowship, a grant from the
                Samsung GRO program, and the Stanford SAIL Toyota Research Center, NSF grant IIS-1763268, KAUST
                grant BAS/1/1685-01-01, and a research gift from Amazon Web Services. They also want to thank Iro
                Armeni, Angel X. Chang, and Jiayun Wang for inspiring discussions and their help in bringing this
                project to fruition. Last but not least, they want to express their gratitude to the wonderful
                Turkers of Amazon Mechanical Turk, whose help in curating the introduced datasets was paramount.
            </p>
        </div>
    </div>


</div>
<script type='text/javascript' id='clustrmaps'
        src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=110&t=tt&d=_kJ7hJdlh3UTdIJueDububmhQbOOTRZpo-A1RUHuEqU&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
</body>

</html>