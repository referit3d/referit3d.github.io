<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ReferIt3D</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="icon" type="image/png" href="img/seal_icon.png">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
<div class="container" id="main">
    <div class="row">

        <h2 class="col-md-12 text-center" style="padding-bottom:20px">

            <b>ReferIt3D: Neural Listeners for Fine-Grained<br>3D Object Identification in Real-World Scenes</br></b>
            <span style="font-size:18pt"> ECCV 2020, Oral </span>
            <br>
        </h2>

    </div>
    <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline" style="font-size:18pt">
                <li>
                    <a href="http://ai.stanford.edu/~optas/">
                        Panos Achlioptas
                    </a>
                    </br>Stanford University
                </li>
                <li>
                    <a href="http://aabdelreheem.me">
                        Ahmed Abdelreheem
                    </a>
                    </br>KAUST
                </li>
                <li>
                    <a href="http://fxia.me/">
                        Fei Xia
                    </a>
                    </br>Stanford University
                </li>

                <br>

                <li>
                    <a href="http://www.mohamed-elhoseiny.com/">
                        Mohamed Elhoseiny
                    </a>
                    </br>Stanford University, KAUST
                </li>
                <li>
                    <a href="https://geometry.stanford.edu/member/guibas/">
                        Leonidas Guibas
                    </a>
                    </br>Stanford University
                </li>
            </ul>
        </div>
    </div>


    <div class="row" style="padding-top:45px">
        <div class="col-md-6 col-md-offset-3 text-center">
            <ul class="nav nav-pills nav-justified">
                <li>
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf">
                        <h4><strong>[ Paper ]</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="#video">
                        <h4><strong>[ Video ]</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/referit3d/referit3d">
                        <h4><strong>[ Code ]</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="#dataset">
                        <h4><strong>[ Dataset ]</strong></h4>
                    </a>
                </li>
            </ul>
        </div>
    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Abstract</b>
            </h3>
            <p class="text-justify">
                In this work we study the problem of using referential language to identify common objects in real-world
                3D scenes. We focus on a challenging setup where the referred object belongs to a fine-grained object
                class and the underlying scene contains multiple object instances of that class. Due to the scarcity and
                unsuitability of existent 3D-oriented linguistic resources for this task, we first develop two
                large-scale and complementary visio-linguistic datasets: i) Sr3D, which contains 83.5K template-based
                utterances leveraging spatial relations among fine-grained object classes to localize a referred object
                in a scene, and ii) Nr3D which contains 41.5K natural, free-form, utterances collected by deploying a
                2-player object reference game in 3D scenes. Using utterances of either datasets, human listeners can
                recognize the referred object with high (>86%, 92% resp.) accuracy. By tapping on this data, we develop
                novel neural listeners that can comprehend object-centric natural language and identify the referred
                object directly in a 3D scene. Our key technical contribution is designing an approach for combining
                linguistic and geometric information (in the form of 3D point clouds) and creating multi-modal (3D)
                neural listeners. We also show that architectures which promote object-to-object communication via graph
                neural networks outperform less context-aware alternatives, and that fine-grained object classification
                is a bottleneck for language-assisted 3D object identification.
            </p>
        </div>
    </div>


    <div class="row" id="video" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Video</b>
            </h3>
<!--            <video id="v0" width="100%" loop="" muted="" controls="">-->
<!--                <source src="img/hi_res.mp4" type="video/mp4">-->
<!--            </video>-->
            <iframe width="100%" height="400"
                src="https://www.youtube.com/embed/yEdf24hF_sY">
            </iframe>

        </div>

    </div>

    <div class="row" id="dataset" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Dataset</b>
            </h3>
            <ul>
                <li> You can download Nr3D <a href="https://drive.google.com/file/d/1qswKclq4BlnHSGMSgzLmUu8iqdUXD8ZC/view">here</a> (10.7MB) and Sr3D/Sr3D+ <a href="https://drive.google.com/drive/folders/1DS4uQq7fCmbJHeE-rEbO8G1-XatGEqNV">here</a> (19MB / 20MB). </li>
            </ul>
            <h4><b>Browse</b></h4>
            <ul>
                <li> You can browse Nr3D on ScanNet using our <a href="./data_browser.html"> browser</a>.</li>
            </ul>

            <br>
        </div>
    </div>

    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Method: ReferIt3DNet</b>
            </h3>
        </div>

        <div class="col-md-8 col-md-offset-2">
            <figure>
                <img src="img/method.png" style="padding-bottom:10px" class="img-responsive" alt="overview">
                    <figcaption>
                    </figcaption>
            </figure>
        </div>

        <div class="col-md-8 col-md-offset-2">
            Each object of a 3D scene, represented as a 6D point cloud containing its xyz coordinates and RGB color,
            is encoded by a visual encoder (e.g., PointNet++), with shared weights. Simultaneously, the utterance
            describing the referred object (e.g., “<i>the armchair next to the whiteboard</i>”) is processed by a Recurrent Neural
            Network (RNN). The resulting representations are fused together and processed by a Dynamic Graph Convolution
            Network (DGCN) which creates an object-centric <i>and</i> scene- (context-) aware representation per object.
            The output of the DGCN is processed by an MLP classifier that estimates the likelihood of each object to be
            the referred one. Two auxiliary losses modulate the unfused representations before these are processed by the DGCN
            via an object-class classifier and a text classifier respectively.
        </div>
    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Qualitative Results</b>
            </h3>
        </div>

        <div class="col-md-8 col-md-offset-2">
            <figure>
                <img src="img/listener_qualitative_res.png" style="padding-bottom:10px" class="img-responsive" alt="overview">
                <figcaption>
                </figcaption>
            </figure>
        </div>

        <div class="col-md-8 col-md-offset-2">
            Successful cases of applying <b><i>ReferIt3DNet</i></b> are shown in the top four images and failure
            ones in the bottom two. Targets are shown in <span style="color:green">green</span> boxes, intra-class
            distractors in <span style="color:red">red</span>, and the referential text is displayed under each image.
            The network predictions are shown inside dashed <span style="color:yellow">yellow</span> circles, along with the
                inferred probabilities. We omit the probabilities of inter-class distractors to ease the presentation.
        </div>
    </div>

    <div class="row" id="citation" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Citation</b>
            </h3>
            If you find our work useful in your research, please consider citing:
<pre class="w3-panel w3-leftbar w3-light-grey">
@article{achlioptas2020referit_3d,
    title={ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes},
    author={Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas},
    journal={16th European Conference on Computer Vision (ECCV)},
    year={2020}
}</pre>
        </div>
    </div>

    <div class="row" id="benchmark" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>ReferIt3D Benchmark Challenge</b>
            </h3>
            Coming soon!
        </div>

    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Acknowledgements</b>
            </h3>
            <p class="text-justify">
                The authors wish to acknowledge the support of a Vannevar Bush Faculty Fellowship, a grant from the
                Samsung GRO program and the Stanford SAIL Toyota Research Center, NSF grant IIS-1763268, KAUST grant
                BAS/1/1685-01-01, and a research gift from Amazon Web Services. The website template was borrowed from
                <a href="http://mgharbi.com/">Michaël Gharbi</a>.
            </p>
        </div>
    </div>


    </div>
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=110&t=tt&d=_kJ7hJdlh3UTdIJueDububmhQbOOTRZpo-A1RUHuEqU&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
    </body>

</html>
